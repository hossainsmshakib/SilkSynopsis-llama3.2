{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d15d8294-3328-4e07-ad16-8a03e9bbfdb9",
   "metadata": {},
   "source": [
    "# Welcome to your first assignment!\n",
    "\n",
    "Instructions are below. Please give this a try, and look in the solutions folder if you get stuck (or feel free to ask me!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ada885d9-4d42-4d9b-97f0-74fbbbfe93a9",
   "metadata": {},
   "source": [
    "<table style=\"margin: 0; text-align: left;\">\n",
    "    <tr>\n",
    "        <td style=\"width: 150px; height: 150px; vertical-align: middle;\">\n",
    "            <img src=\"../resources.jpg\" width=\"150\" height=\"150\" style=\"display: block;\" />\n",
    "        </td>\n",
    "        <td>\n",
    "            <h2 style=\"color:#f71;\">Just before we get to the assignment --</h2>\n",
    "            <span style=\"color:#f71;\">I thought I'd take a second to point you at this page of useful resources for the course. This includes links to all the slides.<br/>\n",
    "            <a href=\"https://edwarddonner.com/2024/11/13/llm-engineering-resources/\">https://edwarddonner.com/2024/11/13/llm-engineering-resources/</a><br/>\n",
    "            Please keep this bookmarked, and I'll continue to add more useful links there over time.\n",
    "            </span>\n",
    "        </td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9fa1fc-eac5-4d1d-9be4-541b3f2b3458",
   "metadata": {},
   "source": [
    "# HOMEWORK EXERCISE ASSIGNMENT\n",
    "\n",
    "Upgrade the day 1 project to summarize a webpage to use an Open Source model running locally via Ollama rather than OpenAI\n",
    "\n",
    "You'll be able to use this technique for all subsequent projects if you'd prefer not to use paid APIs.\n",
    "\n",
    "**Benefits:**\n",
    "1. No API charges - open-source\n",
    "2. Data doesn't leave your box\n",
    "\n",
    "**Disadvantages:**\n",
    "1. Significantly less power than Frontier Model\n",
    "\n",
    "## Recap on installation of Ollama\n",
    "\n",
    "Simply visit [ollama.com](https://ollama.com) and install!\n",
    "\n",
    "Once complete, the ollama server should already be running locally.  \n",
    "If you visit:  \n",
    "[http://localhost:11434/](http://localhost:11434/)\n",
    "\n",
    "You should see the message `Ollama is running`.  \n",
    "\n",
    "If not, bring up a new Terminal (Mac) or Powershell (Windows) and enter `ollama serve`  \n",
    "And in another Terminal (Mac) or Powershell (Windows), enter `ollama pull llama3.2`  \n",
    "Then try [http://localhost:11434/](http://localhost:11434/) again.\n",
    "\n",
    "If Ollama is slow on your machine, try using `llama3.2:1b` as an alternative. Run `ollama pull llama3.2:1b` from a Terminal or Powershell, and change the code below from `MODEL = \"llama3.2\"` to `MODEL = \"llama3.2:1b\"`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e2a9393-7767-488e-a8bf-27c12dca35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "29ddd15d-a3c5-4f4e-a678-873f56162724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "OLLAMA_API = \"http://localhost:11434/api/chat\"\n",
    "HEADERS = {\"Content-Type\": \"application/json\"}\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "dac0a679-599c-441f-9bf2-ddc73d35b940",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a messages list using the same format that we used for OpenAI\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": \"Describe about LLM\"}\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7bb9c624-14f0-4945-a719-8ddb64f66f47",
   "metadata": {},
   "outputs": [],
   "source": [
    "payload = {\n",
    "        \"model\": MODEL,\n",
    "        \"messages\": messages,\n",
    "        \"stream\": False\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "479ff514-e8bd-4985-a572-2ea28bb4fa40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠹ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠸ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠼ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠴ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠦ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠇ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠏ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠋ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest ⠙ \u001b[K\u001b[?25h\u001b[?2026l\u001b[?2026h\u001b[?25l\u001b[1Gpulling manifest \u001b[K\n",
      "pulling dde5aa3fc5ff... 100% ▕████████████████▏ 2.0 GB                         \u001b[K\n",
      "pulling 966de95ca8a6... 100% ▕████████████████▏ 1.4 KB                         \u001b[K\n",
      "pulling fcc5a6bec9da... 100% ▕████████████████▏ 7.7 KB                         \u001b[K\n",
      "pulling a70ff7e570d9... 100% ▕████████████████▏ 6.0 KB                         \u001b[K\n",
      "pulling 56bb8bd477a5... 100% ▕████████████████▏   96 B                         \u001b[K\n",
      "pulling 34bb5ab01051... 100% ▕████████████████▏  561 B                         \u001b[K\n",
      "verifying sha256 digest \u001b[K\n",
      "writing manifest \u001b[K\n",
      "success \u001b[K\u001b[?25h\u001b[?2026l\n"
     ]
    }
   ],
   "source": [
    "# Let's just make sure the model is loaded\n",
    "\n",
    "!ollama pull llama3.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "42b9f644-522d-4e05-a691-56e7658c0ea9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for Large Language Model. It is a type of artificial intelligence (AI) model that is designed to process and understand human language at a large scale. LLMs are trained on vast amounts of text data, which enables them to learn patterns, relationships, and nuances of language.\n",
      "\n",
      " Characteristics of LLM:\n",
      "\n",
      "1. **Large dataset**: LLMs are trained on enormous datasets of text, often in the billions of parameters.\n",
      "2. **Deep learning architecture**: LLMs use deep neural networks with multiple layers to process and transform input text into meaningful representations.\n",
      "3. **Self-supervised learning**: LLMs are typically self-supervised, meaning they learn from their own generated output rather than being labeled by humans.\n",
      "4. **Transformers-based architecture**: Many modern LLMs are based on the transformer architecture, which uses attention mechanisms to weigh importance of different words or tokens in the input text.\n",
      "\n",
      "Applications of LLM:\n",
      "\n",
      "1. **Language translation**: LLMs can be used for machine translation, allowing computers to translate languages with high accuracy.\n",
      "2. **Text summarization**: LLMs can summarize long documents into concise summaries.\n",
      "3. **Question answering**: LLMs can answer questions by understanding the context and meaning of the input text.\n",
      "4. **Chatbots and virtual assistants**: LLMs can be used to power chatbots and virtual assistants, enabling them to understand natural language input and respond accordingly.\n",
      "5. **Text generation**: LLMs can generate text based on a given prompt or topic.\n",
      "\n",
      "Types of LLM:\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: A widely used LLM that uses a combination of pre-training and fine-tuning to achieve state-of-the-art performance.\n",
      "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: An improved version of BERT that uses a different approach to fine-tuning.\n",
      "3. **XLNet**: A variant of BERT that uses a different training objective and achieves better results on certain tasks.\n",
      "\n",
      "Advantages of LLM:\n",
      "\n",
      "1. **Improved accuracy**: LLMs can achieve high accuracy in various NLP tasks, often surpassing human performance.\n",
      "2. **Scalability**: LLMs can process large amounts of text data quickly and efficiently.\n",
      "3. **Flexibility**: LLMs can be fine-tuned for specific tasks and applications.\n",
      "\n",
      "However, LLMs also have some limitations and challenges:\n",
      "\n",
      "1. **Data quality issues**: LLMs are only as good as the data they are trained on, which can lead to biases and inaccuracies.\n",
      "2. **Explainability**: It can be challenging to understand how LLMs arrive at their conclusions, making it difficult to interpret their output.\n",
      "3. **Security concerns**: LLMs can be vulnerable to adversarial attacks and other security threats.\n",
      "\n",
      "Overall, LLMs have revolutionized the field of NLP and have numerous applications in various industries, including healthcare, finance, and customer service.\n"
     ]
    }
   ],
   "source": [
    "# If this doesn't work for any reason, try the 2 versions in the following cells\n",
    "# And double check the instructions in the 'Recap on installation of Ollama' at the top of this lab\n",
    "# And if none of that works - contact me!\n",
    "\n",
    "response = requests.post(OLLAMA_API, json=payload, headers=HEADERS)\n",
    "print(response.json()['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a021f13-d6a1-4b96-8e18-4eae49d876fe",
   "metadata": {},
   "source": [
    "# Introducing the ollama package\n",
    "\n",
    "And now we'll do the same thing, but using the elegant ollama python package instead of a direct HTTP call.\n",
    "\n",
    "Under the hood, it's making the same call as above to the ollama server running at localhost:11434"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7745b9c4-57dc-4867-9180-61fa5db55eb8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for Large Language Model. It is a type of artificial intelligence (AI) model that is designed to process and understand human language at a large scale. LLMs are trained on vast amounts of text data, which enables them to learn patterns and relationships in language, allowing them to generate human-like text, answer questions, and complete tasks.\n",
      "\n",
      "Key Characteristics of LLM:\n",
      "\n",
      "1. **Scalability**: LLMs can handle large volumes of text data and process it quickly, making them ideal for applications that require fast processing and analysis.\n",
      "2. **Contextual Understanding**: LLMs are designed to understand the context in which language is used, allowing them to comprehend nuances and subtleties of human communication.\n",
      "3. **Generative Capability**: LLMs can generate text based on a given prompt or input, making them useful for applications such as writing assistance, chatbots, and language translation.\n",
      "4. **Flexibility**: LLMs can be fine-tuned for specific tasks, allowing developers to adapt them to various use cases and applications.\n",
      "\n",
      "Applications of LLM:\n",
      "\n",
      "1. **Language Translation**: LLMs can be used to translate text from one language to another, enabling communication across linguistic and cultural boundaries.\n",
      "2. **Text Summarization**: LLMs can summarize long documents or articles into concise summaries, making it easier for users to access key information.\n",
      "3. **Chatbots and Virtual Assistants**: LLMs can power chatbots and virtual assistants, enabling them to understand user queries and provide relevant responses.\n",
      "4. **Content Generation**: LLMs can generate high-quality content such as articles, social media posts, and product descriptions.\n",
      "5. **Question Answering**: LLMs can be used to answer questions on a wide range of topics, making them useful for applications such as knowledge bases and information retrieval.\n",
      "\n",
      "Advantages of LLM:\n",
      "\n",
      "1. **Improved Accuracy**: LLMs can provide more accurate results than traditional machine learning models, especially when dealing with natural language inputs.\n",
      "2. **Increased Efficiency**: LLMs can process text data quickly and efficiently, making them ideal for applications that require fast processing times.\n",
      "3. **Enhanced User Experience**: LLMs can generate human-like responses, providing a more engaging and user-friendly experience.\n",
      "\n",
      "However, LLMs also have limitations and challenges, including:\n",
      "\n",
      "1. **Data Quality**: The quality of the data used to train LLMs can significantly impact their performance and accuracy.\n",
      "2. **Bias and Fairness**: LLMs can inherit biases present in the training data, making it essential to ensure that the data is diverse and representative.\n",
      "3. **Explainability**: LLMs can be challenging to interpret and explain, making it difficult to understand why they arrive at certain conclusions or recommendations.\n",
      "\n",
      "Overall, LLMs have revolutionized the field of natural language processing (NLP) and are being used in a wide range of applications, from customer service and content generation to language translation and question answering.\n"
     ]
    }
   ],
   "source": [
    "import ollama\n",
    "\n",
    "response = ollama.chat(model=MODEL, messages=messages)\n",
    "print(response['message']['content'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4704e10-f5fb-4c15-a935-f046c06fb13d",
   "metadata": {},
   "source": [
    "## Alternative approach - using OpenAI python library to connect to Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "23057e00-b6fc-4678-93a9-6b31cb704bff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LLM stands for Large Language Model. It's a type of artificial intelligence (AI) designed to process and understand human language at scale. Here's a detailed overview:\n",
      "\n",
      "**Key characteristics:**\n",
      "\n",
      "1. **Scale**: LLMs are trained on massive amounts of data, often in the order of billions of words or even trillions.\n",
      "2. **Language understanding**: They're capable of understanding the nuances of human language, including grammar, syntax, and semantics.\n",
      "3. **Contextualization**: LLMs can consider multiple context clues to make predictions about the relationship between words, phrases, and sentences.\n",
      "\n",
      "**How it works:**\n",
      "\n",
      "1. **Training data**: LLMs are trained on large datasets of text from various sources, such as books, articles, and online content.\n",
      "2. **Self-supervised learning**: During training, the model tunes itself to predict the next word or phrase in a sequence, considering context and relationships between words.\n",
      "3. **Deep neural networks**: LLMs utilize deep learning techniques, such as recurrent neural networks (RNNs) and transformer architectures, to process sequential data.\n",
      "\n",
      "**Applications:**\n",
      "\n",
      "1. **Language generation**: LLMs can generate coherent text based on input prompts or seeds.\n",
      "2. **Translation**: They're used in machine translation tasks, where the goal is to translate a source language to target language.\n",
      "3. **Text summarization**: LLMs can summarize long documents or articles into concise summaries.\n",
      "4. **Language generation models**: Some applications use LLMs as language generators, producing novel content for entertainment or creative purposes.\n",
      "\n",
      "**Examples of popular LLMs:**\n",
      "\n",
      "1. **BERT (Bidirectional Encoder Representations from Transformers)**: Developed by Google, BERT is a pioneering model in the field of contextualized word embeddings.\n",
      "2. **RoBERTa (Robustly Optimized BERT Pretraining Approach)**: Another popular variant of BERT developed by Facebook and Carnegie Mellon University.\n",
      "\n",
      "**Limitations and future directions:**\n",
      "\n",
      "1. **Training data**: The availability and quality of training data can significantly impact model performance and bias.\n",
      "2. **Adversarial attacks**: LLMs are vulnerable to adversarial attacks, which can be used to manipulate or deceive the model.\n",
      "3. **Explainability and transparency**: Understanding how LLMs make decisions is crucial for trustworthiness and deployment in critical applications.\n",
      "\n",
      "As research continues to improve these models, we'll see advancements in areas like multimodal learning (e.g., vision language), few-shot learning, and more robust evaluation metrics.\n"
     ]
    }
   ],
   "source": [
    "# There's actually an alternative approach that some people might prefer\n",
    "# You can use the OpenAI client python library to call Ollama:\n",
    "\n",
    "from openai import OpenAI\n",
    "ollama_via_openai = OpenAI(base_url='http://localhost:11434/v1', api_key='ollama')\n",
    "\n",
    "response = ollama_via_openai.chat.completions.create(\n",
    "    model=MODEL,\n",
    "    messages=messages\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc7d1de3-e2ac-46ff-a302-3b4ba38c4c90",
   "metadata": {},
   "source": [
    "## Also trying the amazing reasoning model DeepSeek\n",
    "\n",
    "Here we use the version of DeepSeek-reasoner that's been distilled to 1.5B.  \n",
    "This is actually a 1.5B variant of Qwen that has been fine-tuned using synethic data generated by Deepseek R1.\n",
    "\n",
    "Other sizes of DeepSeek are [here](https://ollama.com/library/deepseek-r1) all the way up to the full 671B parameter version, which would use up 404GB of your drive and is far too large for most!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1622d9bb-5c68-4d4e-9ca4-b492c751f898",
   "metadata": {},
   "source": [
    "# NOW the exercise for you\n",
    "\n",
    "Take the code from day1 and incorporate it here, to build a website summarizer that uses Llama 3.2 running locally instead of OpenAI; use either of the above approaches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6de38216-6d1c-48c4-877b-86d403f4e0f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# imports\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from IPython.display import Markdown, display\n",
    "import ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "33cd07b7-998a-4257-bd2c-c3b1603e0a6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants\n",
    "\n",
    "MODEL = \"llama3.2\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0bec085a-b2f0-42c0-86f9-1af2eacbd7f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A class to represent a Webpage\n",
    "\n",
    "class Website:\n",
    "    \"\"\"\n",
    "    A utility class to represent a Website that we have scraped\n",
    "    \"\"\"\n",
    "    url: str\n",
    "    title: str\n",
    "    text: str\n",
    "\n",
    "    def __init__(self, url):\n",
    "        \"\"\"\n",
    "        Create this Website object from the given url using the BeautifulSoup library\n",
    "        \"\"\"\n",
    "        self.url = url\n",
    "        response = requests.get(url)\n",
    "        soup = BeautifulSoup(response.content, 'html.parser')\n",
    "        self.title = soup.title.string if soup.title else \"No title found\"\n",
    "        for irrelevant in soup.body([\"script\", \"style\", \"img\", \"input\"]):\n",
    "            irrelevant.decompose()\n",
    "        self.text = soup.body.get_text(separator=\"\\n\", strip=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b16afb2c-c97e-4af1-89cd-13b00041fe88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Home - Edward Donner\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Well, hi there.\n",
      "I’m Ed. I like writing code and experimenting with LLMs, and hopefully you’re here because you do too. I also enjoy DJing (but I’m badly out of practice), amateur electronic music production (\n",
      "very\n",
      "amateur) and losing myself in\n",
      "Hacker News\n",
      ", nodding my head sagely to things I only half understand.\n",
      "I’m the co-founder and CTO of\n",
      "Nebula.io\n",
      ". We’re applying AI to a field where it can make a massive, positive impact: helping people discover their potential and pursue their reason for being. Recruiters use our product today to source, understand, engage and manage talent. I’m previously the founder and CEO of AI startup untapt,\n",
      "acquired in 2021\n",
      ".\n",
      "We work with groundbreaking, proprietary LLMs verticalized for talent, we’ve\n",
      "patented\n",
      "our matching model, and our award-winning platform has happy customers and tons of press coverage.\n",
      "Connect\n",
      "with me for more!\n",
      "January 23, 2025\n",
      "LLM Workshop – Hands-on with Agents – resources\n",
      "December 21, 2024\n",
      "Welcome, SuperDataScientists!\n",
      "November 13, 2024\n",
      "Mastering AI and LLM Engineering – Resources\n",
      "October 16, 2024\n",
      "From Software Engineer to AI Data Scientist – resources\n",
      "Navigation\n",
      "Home\n",
      "Connect Four\n",
      "Outsmart\n",
      "An arena that pits LLMs against each other in a battle of diplomacy and deviousness\n",
      "About\n",
      "Posts\n",
      "Get in touch\n",
      "ed [at] edwarddonner [dot] com\n",
      "www.edwarddonner.com\n",
      "Follow me\n",
      "LinkedIn\n",
      "Twitter\n",
      "Facebook\n",
      "Subscribe to newsletter\n",
      "Type your email…\n",
      "Subscribe\n"
     ]
    }
   ],
   "source": [
    "# Let's try one out\n",
    "\n",
    "ed = Website(\"https://edwarddonner.com\")\n",
    "print(ed.title)\n",
    "print(ed.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a37c767b-b27c-4523-b71a-6d452e45c300",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our system prompt - you can experiment with this later, changing the last sentence to 'Respond in markdown in Spanish.\"\n",
    "\n",
    "system_prompt = \"You are an assistant that analyzes the contents of a website \\\n",
    "and provides a short summary, ignoring text that might be navigation related. \\\n",
    "Respond in markdown.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "0ef86cb8-dc63-4ff9-b95c-fd72c78fca91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function that writes a User Prompt that asks for summaries of websites:\n",
    "\n",
    "def user_prompt_for(website):\n",
    "    user_prompt = f\"You are looking at a website titled {website.title}\"\n",
    "    user_prompt += \"The contents of this website is as follows; \\\n",
    "please provide a short summary of this website in markdown. \\\n",
    "If it includes news or announcements, then summarize these too.\\n\\n\"\n",
    "    user_prompt += website.text\n",
    "    return user_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f863ebdd-cfbd-409f-980b-635f46756f50",
   "metadata": {},
   "outputs": [],
   "source": [
    "# See how this function creates exactly the format above\n",
    "\n",
    "def messages_for(website):\n",
    "    return [\n",
    "        {\"role\": \"system\", \"content\": system_prompt},\n",
    "        {\"role\": \"user\", \"content\": user_prompt_for(website)}\n",
    "    ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8b71a4c7-baf6-4913-8887-9fff93f21116",
   "metadata": {},
   "outputs": [],
   "source": [
    "# And now: call the Ollama function instead of OpenAI\n",
    "\n",
    "def summarize(url):\n",
    "    website = Website(url)\n",
    "    messages = messages_for(website)\n",
    "    response = ollama.chat(model=MODEL, messages=messages)\n",
    "    return response['message']['content']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b751ab69-1424-4700-8b97-172bc9e61425",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# Website Summary\\n### Overview\\nThe website appears to be the personal blog of Edward Donner, a co-founder and CTO of Nebula.io. He shares his interests in writing code, AI development, music production, and DJing.\\n\\n### Content Highlights\\n\\n* **LLM Workshop**: Upcoming workshop on January 23, 2025, where attendees can work with agents.\\n* **Recent News**:\\n\\t+ December 21, 2024: Announcement for SuperDataScientists community members.\\n\\t+ November 13, 2024: Welcome message to the Mastering AI and LLM Engineering resources page.\\n\\t+ October 16, 2024: Introduction to resources on transitioning from software engineer to AI data scientist.\\n\\n### Additional Information\\n\\n* Contact information: email address `ed [at] edwarddonner [dot] com`, website URL `www.edwarddonner.com`.\\n* Social media links: LinkedIn, Twitter, Facebook.\\n* Newsletter subscription option.'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summarize(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "091c382a-076d-4ee6-aaf7-2cc28b43a838",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A function to display this nicely in the Jupyter output, using markdown\n",
    "\n",
    "def display_summary(url):\n",
    "    summary = summarize(url)\n",
    "    display(Markdown(summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4ffdf19e-5893-4fac-a8c2-499e4a8e31ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary**\n",
       "================\n",
       "\n",
       "* The website is owned by Edward Donner, the co-founder and CTO of Nebula.io.\n",
       "* He is a writer who enjoys experimenting with LLMs, DJing, and amateur electronic music production.\n",
       "* Nebula.io applies AI to help people discover their potential and pursue their reason for being, using proprietary LLMs in talent management.\n",
       "* The website features news and announcements related to the company's projects:\n",
       "\t+ LLM Workshop – Hands-on with Agents (January 23, 2025)\n",
       "\t+ Welcome, SuperDataScientists! (November 13, 2024)\n",
       "\t+ Mastering AI and LLM Engineering – Resources (October 16, 2024)\n",
       "\t+ From Software Engineer to AI Data Scientist – resources\n",
       "* The website also features a \"Connect Four\" game and an \"Outsmart\" arena where LLMs compete in diplomacy and deviousness.\n",
       "\n",
       "**No Navigation Links**\n",
       "------------------------\n",
       "\n",
       "Navigation links are ignored as per your request."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://edwarddonner.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "34ee07ac-cc59-4574-bbf5-0982e865c6cb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "This appears to be a collection of news articles and other content from CNN's website, presented in a scrolling format. The exact topics covered are varied, but some examples include:\n",
       "\n",
       "* Politics:\n",
       "\t+ Trump announces Taiwanese chipmaking giant TSMC to invest $100 billion in US manufacturing\n",
       "\t+ Facebook enables gender discrimination in job ads, European human rights body rules\n",
       "\t+ \"Merit board\" chair was unlawfully fired by Trump, judge rules, keeping her on the job\n",
       "* Business:\n",
       "\t+ Replacing butter for some plant oils could significantly lower risk of mortality, new study finds\n",
       "\t+ What will cost Americans more from sweeping tariffs on Mexico, China and Canada?\n",
       "* Entertainment:\n",
       "\t+ Halle Berry gets 'payback' on Adrien Brody at the Oscars — with a kiss\n",
       "\t+ Gene Hackman wanted to be remembered as a 'decent actor.' He far surpassed that with these roles\n",
       "* Science:\n",
       "\t+ Why those reports of DOGE using AI have experts worried about 'massive risk'\n",
       "\t+ Scientists say this stunning ancient lake in Turkey might as well be on Mars\n",
       "* Health:\n",
       "\t+ Common vaginal condition is really an STD, study finds\n",
       "\t+ 5 things this ER doctor would like everyone to stop doing\n",
       "\n",
       "This content is likely from a CNN news website or app, and may be updated regularly with new articles and information."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://cnn.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b3b98c18-3b26-41de-99eb-9be3e2e42dc3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "**Summary of Anthropic Website**\n",
       "==============================\n",
       "\n",
       "### Introduction to Anthropic\n",
       "\n",
       "Anthropic is an AI safety and research company based in San Francisco, founded by an interdisciplinary team with expertise across machine learning, physics, policy, and product.\n",
       "\n",
       "### Key Announcements\n",
       "\n",
       "* **Claude 3.7 Sonnet**: Released, the most intelligent AI model yet and the first hybrid reasoning model.\n",
       "* **Claude Code**: Launched, an agentic tool for coding.\n",
       "* **Alignment Research**:\n",
       "\t+ \"Constitutional AI: Harmlessness from AI Feedback\" (Dec 15, 2022)\n",
       "\t+ \"Core Views on AI Safety: When, Why, What, and How\" (Mar 8, 2023)\n",
       "\n",
       "### Products and Services\n",
       "\n",
       "* **Claude**: An AI platform for building safe and beneficial applications.\n",
       "* **Claude Enterprise**: A plan for organizations to adopt Claude's technology.\n",
       "* **Claude Code**: An agentic tool for coding.\n",
       "\n",
       "### Research and Commitments\n",
       "\n",
       "* **Transparency**: Responsible scaling policy and security and compliance measures.\n",
       "* **Commitments**: To generate research and create reliable, beneficial AI systems."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display_summary(\"https://anthropic.com\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4682a403-444b-412c-bfec-f54810f36433",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66237a91-357a-42c0-a14d-8eb156f647a8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43a4b4e9-79cb-404c-81f2-058de03aa86f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
